---
title: 'Portfolios: Sensitivity to Inputs'
author: "<br>Kevin Crotty<br>BUSI 448: Investments"
format: 
    revealjs:
        incremental: true
        logo: RiceLogo.png
        footer: "BUSI 448"
        self-contained: true
        code-fold: true
        theme: [default, custom.scss]
        show-slide-number: print
        menu: 
            width: wide
            numbers: true             
execute:
    echo: false
    freeze: auto
jupyter: python3
---
```{python}
#| echo: false
#| execute: true
import numpy as np
from cvxopt import matrix
from cvxopt.solvers import qp as Solver, options as SolverOptions
from scipy.optimize import minimize_scalar

SolverOptions["show_progress"] = False

class portfolio:
    def __init__(self, means, cov, Shorts):
        self.means = np.array(means)
        self.cov = np.array(cov)
        self.Shorts = Shorts
        self.n = len(means)
        if Shorts:
            w = np.linalg.solve(cov, np.ones(self.n))
            self.GMV = w / np.sum(w)
            w = np.linalg.solve(cov, means)
            self.piMu = w / np.sum(w)
        else:
            n = self.n
            Q = matrix(cov, tc="d")
            p = matrix(np.zeros(n), (n, 1), tc="d")
            G = matrix(-np.identity(n), tc="d")
            h = matrix(np.zeros(n), (n, 1), tc="d")
            A = matrix(np.ones(n), (1, n), tc="d")
            b = matrix([1], (1, 1), tc="d")
            sol = Solver(Q, p, G, h, A, b)
            self.GMV = np.array(sol["x"]).flatten() if sol["status"] == "optimal" else np.array(n * [np.nan])

    def frontier(self, m):
        if self.Shorts:
            gmv = self.GMV
            piMu = self.piMu
            m1 = gmv @ self.means
            m2 = piMu @ self.means
            a = (m - m2) / (m1 - m2)
            return a * gmv + (1 - a) * piMu
        else:
            n = self.n
            Q = matrix(self.cov, tc="d")
            p = matrix(np.zeros(n), (n, 1), tc="d")
            G = matrix(-np.identity(n), tc="d")
            h = matrix(np.zeros(n), (n, 1), tc="d")
            A = matrix(np.vstack((np.ones(n), self.means)), (2, n), tc="d")
            b = matrix([1, m], (2, 1), tc="d")
            sol = Solver(Q, p, G, h, A, b)
            return np.array(sol["x"]).flatten() if sol["status"] == "optimal" else np.array(n * [np.nan])

    def tangency(self, r):
        if self.Shorts:
            w = np.linalg.solve(self.cov, self.means - r)
            return w / np.sum(w)
        else:
            def f(m):
                w = self.frontier(m)
                mn = w @ self.means
                sd = np.sqrt(w.T @ self.cov @ w)
                return - (mn - r) / sd
            m = minimize_scalar(f, bounds=[max(r, np.min(self.means)), max(r, np.max(self.means))], method="bounded").x
            return self.frontier(m)

    def optimal(self, raver, rs=None, rb=None):
        n = self.n
        if self.Shorts:
            if (rs or rs==0) and (rb or rb==0):
                Q = np.zeros((n + 2, n + 2))
                Q[2:, 2:] = raver * self.cov
                Q = matrix(Q, tc="d")
                p = np.array([-rs, rb] + list(-self.means))
                p = matrix(p, (n + 2, 1), tc="d")
                G = np.zeros((2, n + 2))
                G[0, 0] = G[1, 1] = -1
                G = matrix(G, (2, n+2), tc="d")
                h = matrix([0, 0], (2, 1), tc="d")
                A = matrix([1, -1] + n*[1], (1, n+2), tc="d")
                b = matrix([1], (1, 1), tc="d")
                sol = Solver(Q, p, G, h, A, b)
                return np.array(sol["x"]).flatten()[2:] if sol["status"] == "optimal" else None
            else:
                w = np.linalg.solve(self.cov, self.means)
                a = np.sum(w)
                return (a/raver)*self.piMu + (1-a/raver)*self.GMV
        else:
           if (rs or rs==0) and (rb or rb==0):
                Q = np.zeros((n + 2, n + 2))
                Q[2:, 2:] = raver * self.cov
                Q = matrix(Q, tc="d")
                p = np.array([-rs, rb] + list(-self.means))
                p = matrix(p, (n+2, 1), tc="d")
                G = matrix(-np.identity(n + 2), tc="d")
                h = matrix(np.zeros(n+2), (n+2, 1), tc="d")
                A = matrix([1, -1] + n * [1], (1, n+2), tc="d")
                b = matrix([1], (1, 1), tc="d")
                sol = Solver(Q, p, G, h, A, b)
                return np.array(sol["x"]).flatten()[2:] if sol["status"] == "optimal" else None
           else:
                Q = matrix(raver * self.cov, tc="d")
                p = matrix(-self.means, (n, 1), tc="d")
                G = matrix(-np.identity(n), tc="d")
                h = matrix(np.zeros(n), (n, 1), tc="d")
                A = matrix(np.ones(n), (1, n), tc="d")
                b = matrix([1], (1, 1), tc="d")
                sol = Solver(Q, p, G, h, A, b)
                return np.array(sol["x"]).flatten() if sol["status"] == "optimal" else None
```

## Where are we?
**Last time**:

::: {.nonincremental}
- Simulation
- Investing over multiple periods
- Rebalancing
:::

. . . 

**Today**:

::: {.nonincremental}
- Sensitivity of mean-variance optimization to inputs
- Dealing with estimation error of inputs
- Empirical and simulated performance
:::


<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->
<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->
# Input sensitivity
<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->
<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->

## Portfolio optimization inputs

- Set of expected returns for assets 

- Set of std deviations (variances) for assets 

- Set of correlations (covariances) across assets 

. . . 

How good are we at estimating these things?

## Three-asset example
``` {python}
import numpy as np
import pandas as pd
import plotly.graph_objects as go
from scipy.optimize import minimize
from cvxopt import matrix
from cvxopt.solvers import qp as Solver, options as SolverOptions

##### Inputs
# Risk-free rate
RF = 0.01

# Expected returns
MNS = np.array([0.06, 0.065, 0.08])

# Standard deviations
SDS = np.array([0.15, 0.165, 0.21])

# Correlations
C  = np.identity(3)
CORR = 0.75
C[0, 1] = C[1, 0] = CORR
C[0, 2] = C[2, 0] = CORR
C[1, 2] = C[2, 1] = CORR

# Covariance matrix
COV = np.diag(SDS) @ C @ np.diag(SDS)


#### Tangency portfolio
w = np.linalg.solve(COV, MNS - RF)
wgts = w / np.sum(w)

# Portfolio expected return
port_expret = wgts @ MNS

# Portfolio standard deviation
port_sd = np.sqrt(wgts @ COV @ wgts)

# Portfolio sharpe ratio
port_sr = (port_expret - RF)/port_sd


##### CAL
WGTS = np.arange(0,1.55,0.05)
cal = pd.DataFrame(dtype='float',columns=['wgt_tangency', 'wgt_rf', 'port_expret', 'port_sd','risk_aversion'], index=np.arange(len(WGTS)))
cal.wgt_tangency = WGTS
cal.wgt_rf = 1-WGTS
cal.port_expret = cal.wgt_tangency*port_expret + cal.wgt_rf*RF
cal.port_sd = np.abs(cal.wgt_tangency)*port_sd
cal.risk_aversion = (port_expret - RF) / (cal.wgt_tangency * port_sd**2)
cal.tail()

##### Frontier
def frontier(means, cov, target):
    n = len(means)
    Q = matrix(cov, tc="d")
    p = matrix(np.zeros(n), (n, 1), tc="d")
    # Constraint: short-sales allowed
    G = matrix(np.zeros((n,n)), tc="d")
    h = matrix(np.zeros(n), (n, 1), tc="d")
    # Fully-invested constraint + E[r] = target
    A = matrix(np.vstack((np.ones(n), means)), (2, n), tc="d")
    b = matrix([1, target], (2, 1), tc="d")
    sol = Solver(Q, p, G, h, A, b)
    wgts = np.array(sol["x"]).flatten() if sol["status"] == "optimal" else np.array(n * [np.nan])
    return wgts
SolverOptions['show_progress'] = False

NUM_TARGETS = 30
df = pd.DataFrame(dtype='float',columns=['target_expret','w1','w2','w3','port_expret','port_sd','sharpe'], index=np.arange(NUM_TARGETS))
df.target_expret = np.linspace(0.05, 0.085,NUM_TARGETS)
for i in df.index:
    wgts = frontier(MNS, COV, df.loc[i,'target_expret'])
    df.loc[i,['w1','w2','w3']] = wgts
    df.loc[i,'port_expret'] = wgts @ MNS
    df.loc[i,'port_sd'] = np.sqrt(wgts @ COV @ wgts)
df.sharpe = (df.port_expret - RF)/df.port_sd


##### Figure
fig = go.Figure()

# Plot frontier
string =  "US Equity: %{customdata[0]:.1%}<br>"
string += "Developed Intl: %{customdata[1]:.1%}<br>"
string += "Emerging Mkt: %{customdata[2]:.1%}<br>"
string += "Sharpe ratio: %{customdata[3]:.4f}<br>"
string += "<extra></extra>"
trace= go.Scatter(x=df.port_sd, y=df.port_expret,mode="lines",
    customdata=df[['w1','w2','w3','sharpe']],hovertemplate=string, name='Frontier')
fig.add_trace(trace)

# Plot underlying assets
trace1= go.Scatter(x=[SDS[0]], y=[MNS[0]],mode="markers", marker=dict(size=10, color="red"), name='US Equity')
fig.add_trace(trace1)
trace2= go.Scatter(x=[SDS[1]], y=[MNS[1]],mode="markers", marker=dict(size=10, color="red"), name='Developed Intl')
fig.add_trace(trace2)
trace3= go.Scatter(x=[SDS[2]], y=[MNS[2]],mode="markers", marker=dict(size=10, color="red"), name='Emerging Mkt')
fig.add_trace(trace3)

# Plot tangency
trace= go.Scatter(x=[port_sd], y=[port_expret] ,
    mode="markers", marker=dict(size=10, color="black"),
    customdata=np.append(wgts,port_sr).reshape(1,4),hovertemplate=string, name='Tangency')
fig.add_trace(trace)

# Plot CAL
string =  "Tangency: %{customdata[0]:.1%}<br>"
string += "Risk-free: %{customdata[1]:.1%}<br>"
string += "Optimal if risk aversion is: %{customdata[2]:.1f}<br>"
string += "<extra></extra>"
trace= go.Scatter(x=cal.port_sd, y=cal.port_expret,mode="lines",marker=dict(color="black"),
    customdata=cal[['wgt_tangency','wgt_rf','risk_aversion']],hovertemplate=string, name='CAL')
fig.add_trace(trace)

# # Plot optimal location on CAL for risk aversion
# trace= go.Scatter(x=[optport_sd], y=[optport_expret],
#     mode="markers", marker=dict(size=15, color="red", symbol='star'),
#     customdata=[[wgt_risky, 1-wgt_risky, RAVER]],hovertemplate=string, name='Optimal Portfolio')
# fig.add_trace(trace)

# Formatting
fig.layout.yaxis["title"] = "Expected Return"
fig.layout.xaxis["title"] = "Standard Deviation"
fig.update_yaxes(tickformat=".1%")
fig.update_xaxes(tickformat=".1%")
fig.update_xaxes(range=[0.7 * df["port_sd"].min(), 1.1 * df["port_sd"].max()])
fig.update_yaxes(range=[0.7 * df["port_expret"].min(), 1.1 * df["port_expret"].max()])
fig.update_layout(legend=dict(yanchor="top", y=0.99, xanchor="left", x=0.01))
fig.show()

```


## Sensitivity to expected returns
```{python}
#| echo: false

def tangency(means, cov, rf, Shorts):
    n = len(means)
    def f(w):
        mn = w @ means
        sd = np.sqrt(w.T @ cov @ w)
        return -(mn - rf) / sd
    # Initial guess (equal-weighted)
    w0 = (1/n)*np.ones(n)
    # Constraint: fully-invested portfolio
    A = np.ones(n)
    b = 1
    cons = [{"type": "eq", "fun": lambda x: A @ x - b}]
    if Shorts==True:
        # No short-sale constraint
        bnds = [(None, None) for i in range(n)] 
    else:
        # With short-sale constraint
        bnds = [(0, None) for i in range(n)] 
    # Optimization
    wgts_tangency = minimize(f, w0, bounds=bnds, constraints=cons).x
    return wgts_tangency

wgts_true = tangency(MNS,COV,RF,Shorts=True)

# Tangency portfolios for a range of assumed asset 1 expected returns
n = len(MNS)
num_grid=200
asset1_means = np.linspace(0.04,0.08,num_grid)
wgts = np.zeros((num_grid,n))

for i,m in enumerate(asset1_means):
    wgts[i] = tangency(np.array([m, MNS[1], MNS[2]]),COV,RF,Shorts=True)
wgt_asset1 = wgts[:,0]
cd = np.empty(shape=(num_grid, n-1,1), dtype=float)
cd[:, 0] = wgts[:,1].reshape(-1, 1)
cd[:, 1] = wgts[:,2].reshape(-1, 1)
string = "US Equity Expected Return Input = %{x:0.2%}<br>"
string +="Tangency Portfolio Weights:<br>"
string += "  US Equity: %{y:0.1%}<br>"
string += "  Developed Intl: %{customdata[0]:.1%}<br>"
string += "  Emerging Mkt: %{customdata[1]:.1%}<br>"
string += "<extra></extra>"
trace = go.Scatter(x=asset1_means,y=wgt_asset1,mode='lines', name="Tangency Weight",
    customdata=cd, hovertemplate=string,
)

# Tangency portfolio at assume input
trace_true = go.Scatter(x=[MNS[0]],y=[wgts_true[0]],mode='markers', name="Tangency Weight at Assumed Input",
    customdata = np.array([[wgts_true[1],wgts_true[2]]]), hovertemplate=string,
)
fig = go.Figure()
fig.add_trace(trace)
# fig.add_trace(trace_true)
fig.layout.xaxis["title"] = "US Equity Expected Return Input"
fig.layout.yaxis["title"] = "US Equity Tangency Portfolio Weight"
fig.update_yaxes(tickformat=".1%")
fig.update_xaxes(tickformat=".1%")
fig.update_layout(legend=dict(yanchor="top", y =0.99, xanchor="left", x=0.01))
fig.show()
```

## Sensitivity to standard deviations
```{python}
#| echo: false
# ##### Variance input of asset 1:
# Tangency portfolios for a range of assumed asset 1 standard deviations
asset1_sds = np.linspace(0.075,0.225,num_grid)
wgts = np.zeros((num_grid,n))

for i,s in enumerate(asset1_sds):
    sds_new = np.array([s, SDS[1], SDS[2]])
    wgts[i] = tangency(MNS,np.diag(sds_new) @ C @ np.diag(sds_new),RF,Shorts=True)
wgt_asset1 = wgts[:,0]
cd = np.empty(shape=(num_grid, n-1,1), dtype=float)
cd[:, 0] = wgts[:,1].reshape(-1, 1)
cd[:, 1] = wgts[:,2].reshape(-1, 1)
string = "US Equity Standard Deviation Input = %{x}<br>"
string +="Tangency Portfolio Weights:<br>"
string += "  US Equity: %{y:0.1%}<br>"
string += "  Developed Intl: %{customdata[0]:.1%}<br>"
string += "  Emerging Mkt: %{customdata[1]:.1%}<br>"
string += "<extra></extra>"
trace = go.Scatter(x=asset1_sds,y=wgt_asset1,mode='lines', name="Tangency Weight",
    customdata=cd, hovertemplate=string,
)

# Tangency portfolio at assumed input
trace_true = go.Scatter(x=[SDS[0]],y=[wgts_true[0]],mode='markers', name="Tangency Weight at Assumed Input",
    customdata = np.array([[wgts_true[1],wgts_true[2]]]), hovertemplate=string,
)
fig = go.Figure()
fig.add_trace(trace)
# fig.add_trace(trace_true)
fig.layout.xaxis["title"] = "US Equity Standard Deviation Input"
fig.layout.yaxis["title"] = "US Equity Tangency Portfolio Weight"
fig.update_yaxes(tickformat=".1%")
fig.update_xaxes(tickformat=".1%")
fig.update_layout(legend=dict(yanchor="top", y =0.99, xanchor="left", x=0.55))
fig.show()

```

## Sensitivity to correlations
```{python}
#| echo: false
##### Correlation of assets 1 and 2:
# Tangency portfolios for a range of assumed asset 1 standard deviations
corr12_grid = np.linspace(0.15,0.95,num_grid)
wgts = np.empty((num_grid,n))

def is_pos_def(x):
    if np.all(np.linalg.eigvals(x) > 0):
        return 'True'
    else:
        return 'False'

for i,c in enumerate(corr12_grid):
    # Covariance matrix
    C[0, 1] = C[1, 0] = c
    # Check feasible correlations
    if is_pos_def(C):
        wgts[i] = tangency(MNS,np.diag(SDS) @ C @ np.diag(SDS),RF,Shorts=True)
    else:
        print("not positive definite" + str(c*100))
wgt_asset1 = wgts[:,0]
cd = np.empty(shape=(num_grid, n-1,1), dtype=float)
cd[:, 0] = wgts[:,1].reshape(-1, 1)
cd[:, 1] = wgts[:,2].reshape(-1, 1)
string = "Input: Correlation of US Equity & Developed Intl = %{x}<br>"
string +="Tangency Portfolio Weights:<br>"
string += "  US Equity: %{y:0.1%}<br>"
string += "  Developed Intl: %{customdata[0]:.1%}<br>"
string += "  Emerging Mkt: %{customdata[1]:.1%}<br>"
string += "<extra></extra>"
trace = go.Scatter(x=corr12_grid,y=wgt_asset1,mode='lines', name="Tangency Weight",
    customdata=cd, hovertemplate=string,
)

# Tangency portfolio at assumed input
trace_true = go.Scatter(x=[CORR],y=[wgts_true[0]],mode='markers', name="Tangency Weight at Assumed Input",
    customdata = np.array([[wgts_true[1],wgts_true[2]]]), hovertemplate=string,
)
fig = go.Figure()
fig.add_trace(trace)
# fig.add_trace(trace_true)
fig.layout.xaxis["title"] = "Correlation of US Equity & Developed Intl"
fig.layout.yaxis["title"] = "US Equity Tangency Portfolio Weight"
fig.update_yaxes(tickformat=".1%")
fig.update_xaxes(tickformat=".1%")
fig.update_layout(legend=dict(yanchor="top", y =0.99, xanchor="left", x=0.55))
fig.show()

```

## The Error-Maximization Problem

Mean-variance portfolio optimization: \

- Will tilt too heavily toward assets with estimated expected returns above true expected returns $(\hat{\mu}>\mu)$ \ 

- Will tilt too heavily toward assets with diversification benefits greater than true benefits $(\widehat{\text{cov}}_{ij}<\text{cov}_{ij})$ \ 

- May try to short assets with diversification benefits lower than true benefits $(\widehat{\text{cov}}_{ij}>\text{cov}_{ij})$




<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->
<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->
# Dealing with Estimation Error
<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->
<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->


## Position limits

#### Short-selling constraints \
&emsp; Prevent hedging positions due to overestimated covariances and/or underestimated $E[r]$

#### Maximum positions \
&emsp; Prevent overweighting due to overestimated $E[r]$ and/or underestimated covariances 


## Shrinkage 

- Shrink extreme inputs toward some more moderate input 
- Example: CAPM betas $$ \beta_{\text{adj}}= 0.67\cdot \hat{\beta} + 0.33\cdot 1 $$
- There are some fairly sophisticated shrinkage techniques for the covariance matrix.



## Use models to infer expected returns
#### Black-Litterman
&emsp; Use market value weights to back out $E[r_i]$'s via CAPM  \
&emsp; Then add alphas to expected returns 

. . .

#### Treynor-Black
&emsp; Consider benchmark index as an asset \
&emsp; Use expected alphas to create an active portfolio \
&emsp; Combine index and active portfolio optimally


## Factor models

- Can be used to estimate both $E[r]$'s and correlations

- **Market Model/CAPM**: $$E[r_i]=r_f + \beta E[r_{\text{mkt}}-r_f]$$ $$\text{cov}_{ij}=\beta_i\beta_j \sigma^2_{\text{mkt}}$$

- Can dramatically reduce the number of estimated parameters

- We will discuss (multi-)factor models beyond CAPM



## Donâ€™t even try to estimate some inputs!
#### Global minimum variance
&emsp; assume all $E[r_i]$'s equal

#### Risk parity
&emsp; assume all $E[r_i]$'s equal and all $\rho_{ij}=0$ \

#### Equal-weighted portfolio
&emsp; assume all $E[r_i]$'s, $\text{sd}[r_i]$'s equal; all  $\rho_{ij}=0$




<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->
<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->
# Empirical Performance of Historical Plug-in Estimators
<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->
<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->

## Historical Plug-in Estimators
#### Expected return
&emsp; use historical arithmetic average return

. . .

#### Standard deviation
&emsp; use historical standard deviation

. . .

#### Correlations 
&emsp; use historical pair-wise correlation

## Stocks, Bonds, and Gold

Let's run a backtest of annual optimization of portfolios of the following asset classes:

- Stocks
- Treasury bonds
- Corporate bonds
- Gold

. . .

We'll use four strategies for input estimation.

## Strategy 1: Est-All 
- use historical data to estimate expected returns, standard deviations, and correlations.  
- optimal risky portfolio is the tangency portfolio
- scale tangency up or down depending on risk aversion or target expected return


## Strategy 2: Est-SD-Corr
- use historical data to estimate standard deviations and correlations
- assume expected returns are the same across all assets.  
- optimal risky portfolio is the global minimum variance portfolio.
- for the purposes of determining optimal capital allocation, use the cross-sectional average of the historical time-series average return as the expected return input.

## Strategy 3: Est-SD
- use historical data to estimate standard deviations only
- assume correlations across assets are zero 
- assume expected returns are the same across all assets
- for the purposes of determining optimal capital allocation, use the cross-sectional average of the historical time-series average return as the expected return input.

## Strategy 4: Est-None
- do not use historical data to estimate expected returns, standard deviations, or correlations. 
- the optimal portfolio is an equal-weighted portfolio of the assets ($1/N$ portfolio). 
- for the purposes of determining optimal capital allocation, use the cross-sectional average of the historical time-series average return as the expected return input.

##
#### To notebook #1


<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->
<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->
# Empirical Performance of Portfolio Constraints
<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->
<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->

## Industry Portfolios

Let's return to our 48 industry portfolio example.

- Using full sample means, standard deviations, and correlations suggested that allowing short selling could improve mean-variance efficiency.

- Let's consider how this would have fared in an **out-of-sample** context.

- We will use expanding windows to estimate inputs.

## 
#### To notebook #2

<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->
<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->
# Simulated Performance of Historical Plug-in Estimators
<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->
<!-- xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx -->


## Length of Estimation Window

- Use last $T$ years to estimate inputs (rebalance each year)

- Consider windows of 10, 20, 30, 40, and 50 years

- Scenarios with more or less dispersion in true expected returns

## Higher $E[r]$ dispersion

``` {python}
sim_results =pd.read_csv('../data/sim_results_vary_window.csv')
stats = sim_results.groupby(['means', 'sds','corrs','window']).mean()
stats = stats[['true','est_all', 'est_sd_corr', 'est_sd','est_none']]
w1 = 10
w2 = 20
w3 = 30
w4 = 40
w5 = 50
window_dict = {'w1': w1, 'w2': w2, 'w3': w3, 'w4': w4, 'w5': w5 }
def compare_plot(mns,sds,corr):
    newdf = stats.loc[(mns,sds,corr,slice(None))].stack().reset_index()
    newdf.columns=['window','strategy','sr']
    label_dict = {'true':'True',
                'est_none': 'Est-None',
                'est_all': 'Est-All',
                'est_sd_corr': 'Est-SD-Corr',
                'est_sd': 'Est-SD'}

    newdf['strategy'] = newdf['strategy'].apply(lambda y: label_dict[y])
    newdf['window'] = newdf['window'].apply(lambda y: window_dict[y])
    # newdf
    import plotly.express as px
    fig = go.Figure()
    fig = px.histogram(newdf, x="strategy", y="sr",
                color='window', barmode='group', histfunc='avg',
                height=400)
    fig.layout.yaxis["title"] = "Sharpe ratio"
    fig.layout.xaxis["title"] = "Strategy"             
    fig.show()
compare_plot('mns1','sds1','c1')
```


## Lower $E[r]$ dispersion

``` {python}
compare_plot('mns2','sds1','c1')
```


## Number of Assets

- 3, 5, or 10 assets

- Estimation window of 30 years

- Investment period of 50 years

- Theoretical Sharpe ratio of tangency portfolio is the same

## Number of Assets

``` {python}
sim_results = pd.read_csv('../data/sim_results_vary_nassets.csv')
stats = sim_results.groupby(['n_assets']).mean()
stats = stats.reset_index()
stats['num'] = stats['n_assets'].apply(lambda x: int(x))
stats = stats.sort_values('num')
stats = stats[['num','true','est_all', 'est_sd_corr', 'est_sd','est_none']]
import plotly.express as px
newdf = stats.set_index('num').stack().reset_index()
newdf.columns=['n_assets','strategy','sr']
label_dict = {'true':'True',
            'est_none': 'Est-None',
            'est_all': 'Est-All',
            'est_sd_corr': 'Est-SD-Corr',
            'est_sd': 'Est-SD'}

newdf['strategy'] = newdf['strategy'].apply(lambda y: label_dict[y])
fig = go.Figure()
fig = px.histogram(newdf, x="strategy", y="sr",
            color='n_assets', barmode='group', histfunc='avg',
            height=400)
fig.layout.yaxis["title"] = "Sharpe ratio"
fig.layout.xaxis["title"] = "Strategy"             
fig.show()

```

<!-- 
## Another Solution: Sort on Predictive Signals

We will see this later in the course -->


## For next time: Market Model Regression

<br><br><br><br>

![](RiceLogo.png){fig-align="center"}