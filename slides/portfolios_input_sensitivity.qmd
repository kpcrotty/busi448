---
title: 'Portfolios: Sensitivity of Inputs'
author: "<br>Kevin Crotty<br>BUSI 448: Investments"
format: 
    revealjs:
        incremental: true
        logo: RiceLogo.png
        footer: "BUSI 448"
        self-contained: true
        code-fold: true
        theme: [default, custom.scss]
        show-slide-number: print
execute:
    cache: true
    freeze: auto
    echo: true
    freeze: auto
jupyter: python3
---
```{python}
#| echo: false
#| execute: true
import numpy as np
from cvxopt import matrix
from cvxopt.solvers import qp as Solver, options as SolverOptions
from scipy.optimize import minimize_scalar

SolverOptions["show_progress"] = False

class portfolio:
    def __init__(self, means, cov, Shorts):
        self.means = np.array(means)
        self.cov = np.array(cov)
        self.Shorts = Shorts
        self.n = len(means)
        if Shorts:
            w = np.linalg.solve(cov, np.ones(self.n))
            self.GMV = w / np.sum(w)
            w = np.linalg.solve(cov, means)
            self.piMu = w / np.sum(w)
        else:
            n = self.n
            Q = matrix(cov, tc="d")
            p = matrix(np.zeros(n), (n, 1), tc="d")
            G = matrix(-np.identity(n), tc="d")
            h = matrix(np.zeros(n), (n, 1), tc="d")
            A = matrix(np.ones(n), (1, n), tc="d")
            b = matrix([1], (1, 1), tc="d")
            sol = Solver(Q, p, G, h, A, b)
            self.GMV = np.array(sol["x"]).flatten() if sol["status"] == "optimal" else np.array(n * [np.nan])

    def frontier(self, m):
        if self.Shorts:
            gmv = self.GMV
            piMu = self.piMu
            m1 = gmv @ self.means
            m2 = piMu @ self.means
            a = (m - m2) / (m1 - m2)
            return a * gmv + (1 - a) * piMu
        else:
            n = self.n
            Q = matrix(self.cov, tc="d")
            p = matrix(np.zeros(n), (n, 1), tc="d")
            G = matrix(-np.identity(n), tc="d")
            h = matrix(np.zeros(n), (n, 1), tc="d")
            A = matrix(np.vstack((np.ones(n), self.means)), (2, n), tc="d")
            b = matrix([1, m], (2, 1), tc="d")
            sol = Solver(Q, p, G, h, A, b)
            return np.array(sol["x"]).flatten() if sol["status"] == "optimal" else np.array(n * [np.nan])

    def tangency(self, r):
        if self.Shorts:
            w = np.linalg.solve(self.cov, self.means - r)
            return w / np.sum(w)
        else:
            def f(m):
                w = self.frontier(m)
                mn = w @ self.means
                sd = np.sqrt(w.T @ self.cov @ w)
                return - (mn - r) / sd
            m = minimize_scalar(f, bounds=[max(r, np.min(self.means)), max(r, np.max(self.means))], method="bounded").x
            return self.frontier(m)

    def optimal(self, raver, rs=None, rb=None):
        n = self.n
        if self.Shorts:
            if (rs or rs==0) and (rb or rb==0):
                Q = np.zeros((n + 2, n + 2))
                Q[2:, 2:] = raver * self.cov
                Q = matrix(Q, tc="d")
                p = np.array([-rs, rb] + list(-self.means))
                p = matrix(p, (n + 2, 1), tc="d")
                G = np.zeros((2, n + 2))
                G[0, 0] = G[1, 1] = -1
                G = matrix(G, (2, n+2), tc="d")
                h = matrix([0, 0], (2, 1), tc="d")
                A = matrix([1, -1] + n*[1], (1, n+2), tc="d")
                b = matrix([1], (1, 1), tc="d")
                sol = Solver(Q, p, G, h, A, b)
                return np.array(sol["x"]).flatten()[2:] if sol["status"] == "optimal" else None
            else:
                w = np.linalg.solve(self.cov, self.means)
                a = np.sum(w)
                return (a/raver)*self.piMu + (1-a/raver)*self.GMV
        else:
           if (rs or rs==0) and (rb or rb==0):
                Q = np.zeros((n + 2, n + 2))
                Q[2:, 2:] = raver * self.cov
                Q = matrix(Q, tc="d")
                p = np.array([-rs, rb] + list(-self.means))
                p = matrix(p, (n+2, 1), tc="d")
                G = matrix(-np.identity(n + 2), tc="d")
                h = matrix(np.zeros(n+2), (n+2, 1), tc="d")
                A = matrix([1, -1] + n * [1], (1, n+2), tc="d")
                b = matrix([1], (1, 1), tc="d")
                sol = Solver(Q, p, G, h, A, b)
                return np.array(sol["x"]).flatten()[2:] if sol["status"] == "optimal" else None
           else:
                Q = matrix(raver * self.cov, tc="d")
                p = matrix(-self.means, (n, 1), tc="d")
                G = matrix(-np.identity(n), tc="d")
                h = matrix(np.zeros(n), (n, 1), tc="d")
                A = matrix(np.ones(n), (1, n), tc="d")
                b = matrix([1], (1, 1), tc="d")
                sol = Solver(Q, p, G, h, A, b)
                return np.array(sol["x"]).flatten() if sol["status"] == "optimal" else None
```

## Where are we?
**Last time**:

::: {.nonincremental}
- Investing over multiple periods
- Rebalancing
:::

. . . 

**Today**:

::: {.nonincremental}
- Optimization and input sensitivity
- Estimation of inputs

:::

## Portfolio optimization inputs

- Set of expected returns for assets \<br />

- Set of variances for assets \<br />

- Set of covariances (correlations) across assets \<br />

How good are we at estimating these things?


## Three-asset example
``` {python}
#| echo: false
import numpy as np
import plotly.graph_objects as go

#Based on French return series
# Asset 1: US Equity
# Asset 2: Developed International
# Asset 1: Emerging Market
mn1= 6
mn2= 6.5
mn3= 8
sd1= 15
sd2= 16.5 
sd3= 21
corr12 = 75 
corr13 = 75 
corr23 = 75 

def is_pos_def(x):
    if np.all(np.linalg.eigvals(x) > 0):
        return 'True'
    else:
        return 'False'
    
def figtbl(mn1, mn2, mn3, sd1, sd2, sd3, c12, c13, c23, asset_names):
    mns = np.array((mn1, mn2, mn3)) / 100
    sds = np.array((sd1, sd2, sd3)) / 100
    corr = np.identity(3)
    corr[0,1] = corr[1,0] = c12 / 100
    corr[0,2] = corr[2,0] = c13 / 100
    corr[1,2] = corr[2,1] = c23 / 100
    cov = np.diag(sds) @ corr @ np.diag(sds)
    N = 3

    def custom(string, ports):
        cd = np.empty(shape=(len(ports), N, 1), dtype=float)
        for i in range(N):
            cd[:, i] = np.array([w[i] for w in ports]).reshape(-1, 1)
        string += "<br>"
        for i in range(N):
            string += asset_names[i]
            string += ": %{customdata["
            string += str(i)
            string += "]:.1%}<br>"
        string += "<extra></extra>"
        return string, cd

    P = portfolio(mns, cov, True)

    mingrid = 0.9*np.min(mns)
    maxgrid = 1.025*np.max(mns)
    mnsFrontier = np.linspace(mingrid, maxgrid, 50)
    portsFrontier = [P.frontier(m) for m in mnsFrontier]
    sdsFrontier = [np.sqrt(w @ cov @ w) for w in portsFrontier]


    string, cd = custom('frontier', portsFrontier)
    # string, cd = custom('frontier w/o short sales', portsFrontier)
    trace1 = go.Scatter(
        x=sdsFrontier,
        y=mnsFrontier,
        mode="lines",
        customdata=cd,
        hovertemplate=string,
        line=dict(color="blue"),
        name="3-Fund Frontier"
    )

    gmv = P.GMV

    string = "GMV portfolio<br>"
    for i in range(N):
        string += asset_names[i]+": " + f'{gmv[i]:.1%}' + "<br>"
    string += "<extra></extra>"
    trace1a = go.Scatter(
        x=[np.sqrt(gmv @ cov @ gmv)],
        y=[gmv @ mns],
        mode="markers",
        hovertemplate=string,        
        marker=dict(size=15, color="blue"),
        name="Global Minimum Variance Portfolio"
    )

    trace3 = go.Scatter(
        x=sds,
        y=mns,
        # text=[1, 2, 3],
        # hovertemplate="Asset %{text}<extra></extra>",
        text=asset_names,
        hovertemplate="%{text}<extra></extra>",
        mode="markers",
        marker=dict(size=15, color="red"),
        name="Underlying Assets"
    )
 
    fig = go.Figure()  
    for trace in (trace1, trace1a, trace3):
        fig.add_trace(trace)
    #trace1=frontier w/o short sales
    #trace1a=GMV portfolio w/o short sales
    #trace3=Assets
        
        
    fig.layout.xaxis["title"] = "Standard Deviation"
    fig.layout.yaxis["title"] = "Expected Return"
    # fig.update_xaxes(range=[0, 1.25 * np.max(sds)])
    # fig.update_yaxes(range=[0, 1.25 * np.max(mns)])
    fig.update_yaxes(tickformat=".1%")
    fig.update_xaxes(tickformat=".1%")
    fig.update_layout(legend=dict(yanchor="top", y =0.99, xanchor="left", x=0.01))
    fig.show()
    # return largefig(fig), is_pos_def(cov)
figtbl(mn1, mn2, mn3, sd1, sd2, sd3, corr12, corr13, corr23, ["US Equity", "Developed Intl", "Emerging Mkt"])
```

## Sensitivity to expected returns
```{python}
#| echo: false
import numpy as np
from cvxopt import matrix
from cvxopt.solvers import qp as Solver, options as SolverOptions
from scipy.optimize import minimize_scalar
from scipy.optimize import minimize
import plotly.graph_objects as go
##### Inputs
# Risk-free rate
r = 0.02
# Expected returns
means = np.array([0.06, 0.065, 0.08])
# Standard deviations
sds = np.array([0.15, 0.165, 0.21])
# Correlations
corr12 = 0.75
corr13 = 0.75
corr23 = 0.75
# Covariance matrix
C  = np.identity(3)
C[0, 1] = C[1, 0] = corr12
C[0, 2] = C[2, 0] = corr13
C[1, 2] = C[2, 1] = corr23
cov = np.diag(sds) @ C @ np.diag(sds)

def tangency(means, cov, rf, Shorts):
    n = len(means)
    def f(w):
        mn = w @ means
        sd = np.sqrt(w.T @ cov @ w)
        return -(mn - rf) / sd
    # Initial guess (equal-weighted)
    w0 = (1/n)*np.ones(n)
    # Constraint: fully-invested portfolio
    A = np.ones(n)
    b = 1
    cons = [{"type": "eq", "fun": lambda x: A @ x - b}]
    if Shorts==True:
        # No short-sale constraint
        bnds = [(None, None) for i in range(n)] 
    else:
        # With short-sale constraint
        bnds = [(0, None) for i in range(n)] 
    # Optimization
    wgts_tangency = minimize(f, w0, bounds=bnds, constraints=cons).x
    return wgts_tangency

wgts_true = tangency(means,cov,r,Shorts=True)

# Tangency portfolios for a range of assumed asset 1 expected returns
n = len(means)
num_grid=100
asset1_means = np.linspace(0.04,0.10,num_grid)
wgts = np.zeros((num_grid,n))

for i,m in enumerate(asset1_means):
    wgts[i] = tangency(np.array([m, means[1], means[2]]),cov,r,Shorts=True)
wgt_asset1 = wgts[:,0]
cd = np.empty(shape=(num_grid, n-1,1), dtype=float)
cd[:, 0] = wgts[:,1].reshape(-1, 1)
cd[:, 1] = wgts[:,2].reshape(-1, 1)
string = "Asset 1 Expected Return Input = %{x:0.2%}<br>"
string +="Tangency Portfolio Weights:<br>"
string += "  Asset 1: %{y:0.1%}<br>"
string += "  Asset 2: %{customdata[0]:.1%}<br>"
string += "  Asset 3: %{customdata[1]:.1%}<br>"
string += "<extra></extra>"
trace = go.Scatter(x=asset1_means,y=wgt_asset1,mode='lines', name="Tangency Weight",
    customdata=cd, hovertemplate=string,
)

# Tangency portfolio at assume input
trace_true = go.Scatter(x=[means[0]],y=[wgts_true[0]],mode='markers', name="Tangency Weight at Assumed Input",
    customdata = np.array([[wgts_true[1],wgts_true[2]]]), hovertemplate=string,
)
fig = go.Figure()
fig.add_trace(trace)
# fig.add_trace(trace_true)
fig.layout.xaxis["title"] = "Asset 1 Expected Return Input"
fig.layout.yaxis["title"] = "Asset 1 Tangency Portfolio Weight"
fig.update_yaxes(tickformat=".1%")
fig.update_xaxes(tickformat=".1%")
fig.update_layout(legend=dict(yanchor="top", y =0.99, xanchor="left", x=0.01))
fig.show()

```

## Sensitivity to standard deviations
```{python}
#| echo: false
# ##### Variance input of asset 1:
# Tangency portfolios for a range of assumed asset 1 standard deviations
asset1_sds = np.linspace(0.05,0.25,num_grid)
wgts = np.zeros((num_grid,n))

for i,s in enumerate(asset1_sds):
    sds_new = np.array([s, sds[1], sds[2]])
    wgts[i] = tangency(means,np.diag(sds_new) @ C @ np.diag(sds_new),r,Shorts=True)
wgt_asset1 = wgts[:,0]
cd = np.empty(shape=(num_grid, n-1,1), dtype=float)
cd[:, 0] = wgts[:,1].reshape(-1, 1)
cd[:, 1] = wgts[:,2].reshape(-1, 1)
string = "Asset 1 Standard Deviation Input = %{x}<br>"
string +="Tangency Portfolio Weights:<br>"
string += "  Asset 1: %{y:0.1%}<br>"
string += "  Asset 2: %{customdata[0]:.1%}<br>"
string += "  Asset 3: %{customdata[1]:.1%}<br>"
string += "<extra></extra>"
trace = go.Scatter(x=asset1_sds,y=wgt_asset1,mode='lines', name="Tangency Weight",
    customdata=cd, hovertemplate=string,
)

# Tangency portfolio at assumed input
trace_true = go.Scatter(x=[sds[0]],y=[wgts_true[0]],mode='markers', name="Tangency Weight at Assumed Input",
    customdata = np.array([[wgts_true[1],wgts_true[2]]]), hovertemplate=string,
)
fig = go.Figure()
fig.add_trace(trace)
# fig.add_trace(trace_true)
fig.layout.xaxis["title"] = "Asset 1 Standard Deviation Input"
fig.layout.yaxis["title"] = "Asset 1 Tangency Portfolio Weight"
fig.update_yaxes(tickformat=".1%")
fig.update_xaxes(tickformat=".1%")
fig.update_layout(legend=dict(yanchor="top", y =0.99, xanchor="left", x=0.55))
fig.show()

```

## Sensitivity to correlations
```{python}
#| echo: false
##### Correlation of assets 1 and 2:
# Tangency portfolios for a range of assumed asset 1 standard deviations
corr12_grid = np.linspace(0.15,0.95,num_grid)
wgts = np.empty((num_grid,n))

def is_pos_def(x):
    if np.all(np.linalg.eigvals(x) > 0):
        return 'True'
    else:
        return 'False'

for i,c in enumerate(corr12_grid):
    # Covariance matrix
    C  = np.identity(3)
    C[0, 1] = C[1, 0] = c
    C[0, 2] = C[2, 0] = corr13
    C[1, 2] = C[2, 1] = corr23
    # Check feasible correlations
    if is_pos_def(C):
        wgts[i] = tangency(means,np.diag(sds) @ C @ np.diag(sds),r,Shorts=True)
    else:
        print("not positive definite" + str(c*100))
wgt_asset1 = wgts[:,0]
cd = np.empty(shape=(num_grid, n-1,1), dtype=float)
cd[:, 0] = wgts[:,1].reshape(-1, 1)
cd[:, 1] = wgts[:,2].reshape(-1, 1)
string = "Input: Correlation of Assets 1 and 2 = %{x}<br>"
string +="Tangency Portfolio Weights:<br>"
string += "  Asset 1: %{y:0.1%}<br>"
string += "  Asset 2: %{customdata[0]:.1%}<br>"
string += "  Asset 3: %{customdata[1]:.1%}<br>"
string += "<extra></extra>"
trace = go.Scatter(x=corr12_grid,y=wgt_asset1,mode='lines', name="Tangency Weight",
    customdata=cd, hovertemplate=string,
)

# Tangency portfolio at assumed input
trace_true = go.Scatter(x=[corr12],y=[wgts_true[0]],mode='markers', name="Tangency Weight at Assumed Input",
    customdata = np.array([[wgts_true[1],wgts_true[2]]]), hovertemplate=string,
)
fig = go.Figure()
fig.add_trace(trace)
# fig.add_trace(trace_true)
fig.layout.xaxis["title"] = "Correlation of Assets 1 and 2"
fig.layout.yaxis["title"] = "Asset 1 Tangency Portfolio Weight"
fig.update_yaxes(tickformat=".1%")
fig.update_xaxes(tickformat=".1%")
fig.update_layout(legend=dict(yanchor="top", y =0.99, xanchor="left", x=0.55))
fig.show()

```

## How good are we at estimation? 

- Expected returns

- Standard deviations

- Correlations

## The Error-Maximization Problem

Mean-variance portfolio optimization: \

- Will tilt too heavily toward assets with estimated expected returns above true expected returns $(\hat{\mu}>\mu)$ \ 

- Will tilt too heavily toward assets with diversification benefits greater than true benefits $(\widehat{\text{cov}}_{ij}<\text{cov}_{ij})$ \ 

- May try to short assets with diversification benefits lower than true benefits $(\widehat{\text{cov}}_{ij}>\text{cov}_{ij})$


## Strategies For Dealing with Estimation Error

#### Shrinkage \
&emsp; Shrink extreme inputs toward some more moderate input 

#### Short-selling constraints: \
&emsp; Prevent hedging positions due to overestimated covariances or underestimated expected returns

#### Maximum position limits \
&emsp; Prevents overweighting due to overestimated $E[r]$ or underestimated covariances 

## Strategies Dealing with Estimation Error

#### Apply models to infer expected returns \
&emsp; Use market wgts to back out $E[r_i]$'s via CAPM (Black-Litterman) \
&emsp; Use more advanced factor models to estiamte $E[r_i]$'s <br />

#### Donâ€™t even try to estimate some inputs \
&emsp; Global minimum variance:	assume all $E[r_i]$'s equal \
&emsp; Risk parity: assume all $E[r_i]$'s equal and all $\rho_{ij}=0$ \
&emsp; Equal-weighted portfolio: assume all $E[r_i]$'s, $\text{sd}[r_i]$'s equal; all  $\rho_{ij}=0$


## Performance of Historical Plug-in Estimators

- Expected return: use historical arithmetic average return

- Standard deviation: use historical standard deviation

- Correlations: historical pair-wise correlation

## Option 1: Est-All 

- use historical data to estimate expected returns, standard deviations, and correlations.  

- optimal risky portfolio is the tangency portfolio

- scale tangency up or down depending on risk aversion


## Option 2: Est-SD-Corr

- use historical data to estimate standard deviations and correlations

- assume expected returns are the same across all assets.  

- optimal risky portfolio is the global minimum variance portfolio.

- for the purposes of determining optimal capital allocation based on risk aversion, we use the cross-sectional average of the historical time-series average return.

## Option 3: Est-SD

- use historical data to estimate standard deviations only

- assume correlations across assets are zero 

- assume expected returns are the same across all assets

- use the cross-sectional average of the historical time-series average return as the expected return input for the purposes of determining optimal capital allocation based on risk aversion. 

## Option 4: Est-None

- do not use historical data to estimate expected returns, standard deviations, or correlations. 

- the optimal portfolio is an equal-weighted portfolio of the assets ($1/N$ portfolio). 

- For the purposes of determining optimal capital allocation based on risk aversion, we use the cross-sectional averages of the historical time-series return mean and standard deviation as the expected return and standard deviation inputs. 

## Length of Estimation Window

- Use last $T$ years to estimate inputs (rebalance each year)

- Consider windows of 10, 20, 30, 40, and 50 years

- Scenarios with more or less dispersion in true expected returns

## Length of Estimation Window (high $E[r]$ dispersion)

```{python}
#| echo: false

import numpy as np
from cvxopt import matrix
from cvxopt.solvers import qp as Solver, options as SolverOptions
from scipy.optimize import minimize_scalar
from scipy.optimize import minimize
import plotly.graph_objects as go
from scipy.stats import multivariate_normal as mvn
import pandas as pd

def tangency(means, cov, rf, short_lb):
    '''
    short_lb: lower bound on position weights
    examples: 0  = no short-selling
              -1 = no more than -100% in a given asset
              None=no restrictions on short-selling
    '''

    n = len(means)
    def f(w):
        mn = w @ means
        sd = np.sqrt(w.T @ cov @ w)
        return -(mn - rf) / sd
    # Initial guess (equal-weighted)
    w0 = (1/n)*np.ones(n)
    # Constraint: fully-invested portfolio
    A = np.ones(n)
    b = 1
    cons = [{"type": "eq", "fun": lambda x: A @ x - b}]
    bnds = [(short_lb, None) for i in range(n)] 
    # Optimization
    wgts_tangency = minimize(f, w0, bounds=bnds, constraints=cons).x
    return wgts_tangency

def gmv(cov, short_lb): 
    '''
    short_lb: lower bound on position weights
    examples: 0  = no short-selling
              -1 = no more than -100% in a given asset
              None=no restrictions on short-selling
    '''    
    n = len(cov)
    Q = matrix(cov, tc="d")
    p = matrix(np.zeros(n), (n, 1), tc="d")
    if short_lb==None:
        # No position limits
        G = matrix(np.zeros((n,n)), tc="d")
        h = matrix(np.zeros(n), (n, 1), tc="d")
    else:
        # Constraint: short-sales not allowed
        G = matrix(-np.identity(n), tc="d")
        h = matrix(-short_lb * np.ones(n), (n, 1), tc="d")
    # Fully-invested constraint
    A = matrix(np.ones(n), (1, n), tc="d")
    b = matrix([1], (1, 1), tc="d")
    sol = Solver(Q, p, G, h, A, b, options={'show_progress': False})
    wgts_gmv = np.array(sol["x"]).flatten() if sol["status"] == "optimal" else np.array(n * [np.nan])
    return wgts_gmv

# Simulation function
def simulation(means, cov, short_lb, seed, window):
	rets = mvn.rvs(means, cov, size=window+T, random_state = seed)
	df = pd.DataFrame(data=rets, columns=['r0','r1','r2'])
	df.columns
	df['mn0']=df['r0'].rolling(window).mean()
	df['mn1']=df['r1'].rolling(window).mean()
	df['mn2']=df['r2'].rolling(window).mean()
	df['sd0']=df['r0'].rolling(window).std()
	df['sd1']=df['r1'].rolling(window).std()
	df['sd2']=df['r2'].rolling(window).std()

	corrs = df[['r0','r1','r2']].rolling(window, min_periods=window).corr()
	df['c01']=corrs.loc[(slice(None),'r0'),'r1'].values
	df['c02']=corrs.loc[(slice(None),'r0'),'r2'].values
	df['c12']=corrs.loc[(slice(None),'r1'),'r2'].values
    
	wgts_true = tangency(means,cov,r,short_lb)
	wgt_cal_true = (wgts_true @ means - r) / (raver * (wgts_true @ cov @ wgts_true))


	model_list = ['true', 'est_none', 'est_all', 'est_sd_corr', 'est_sd']
	for model in model_list:
		df['portret_'+model] = np.nan  # portret is the realized portfolio return of the 100% risky asset portfolio
		if model not in ['true','est_none']:
			df['wgt0_'+model] = np.nan
			df['wgt1_'+model] = np.nan
			df['wgt2_'+model] = np.nan
		df['wgt_cal_'+model] =np.nan
		df['raver_portret_'+model] =np.nan #raver_portret_ is the realized return of the CAL choice of the raver investor

	for i in np.arange(window,window+T):
		# Full estimation inputs at each point in time
		means = df[['mn0','mn1','mn2']].iloc[i-1].values
		sds   = df[['sd0','sd1','sd2']].iloc[i-1].values
		corr01 = df.loc[i-1,'c01']
		corr02 = df.loc[i-1,'c02']
		corr12 = df.loc[i-1,'c12']
		C  = np.identity(3)
		C[0, 1] = C[1, 0] = corr01
		C[0, 2] = C[2, 0] = corr02
		C[1, 2] = C[2, 1] = corr12
		cov = np.diag(sds) @ C @ np.diag(sds)

		##### Note: all portfolio weights considered to be beginning of period weights
		##### (so multiply by contemporaneous realized returns)
		# Theoretical optimal weights
		model = 'true'
		df.loc[i,'portret_'+model]= df.loc[i,['r0','r1','r2']].values @ wgts_true
		df.loc[i,'raver_portret_'+model] = r + wgt_cal_true * (df.loc[i,'portret_'+model] -r)

		# Full estimation tangency portfolio
		model = 'est_all'
		w0, w1, w2 = tangency(means,cov,r,short_lb)
		df.loc[i,'wgt0_' + model] = w0
		df.loc[i,'wgt1_' + model] = w1
		df.loc[i,'wgt2_' + model] = w2
		# df.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ df.loc[i,['wgt0_'+model,'wgt1_'+model,'wgt2_'+model]].values
		wgts = np.array([w0, w1, w2])
		df.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ wgts
		df.loc[i,'wgt_cal_'+model] = (wgts @ means - r) / (raver * (wgts @ cov @ wgts))
		df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])
		df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)

		# Estimate only covariance matrix
		model = 'est_sd_corr'
		w0, w1, w2 = gmv(cov,short_lb)
		df.loc[i,'wgt0_' + model] = w0
		df.loc[i,'wgt1_' + model] = w1
		df.loc[i,'wgt2_' + model] = w2
		# df.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ df.loc[i,['wgt0_'+model,'wgt1_'+model,'wgt2_'+model]].values
		wgts = np.array([w0, w1, w2])
		df.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ wgts
		df.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))
		df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])
		df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)


		# Estimate only standard deviations in covariance matrix
		model = 'est_sd'
		cov[0, 1] = cov[1, 0] = 0.0
		cov[0, 2] = cov[2, 0] = 0.0
		cov[1, 2] = cov[2, 1] = 0.0
		w0, w1, w2 = gmv(cov,short_lb)
		df.loc[i,'wgt0_' + model] = w0
		df.loc[i,'wgt1_' + model] = w1
		df.loc[i,'wgt2_' + model] = w2
		# df.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ df.loc[i,['wgt0_'+model,'wgt1_'+model,'wgt2_'+model]].values
		wgts = np.array([w0, w1, w2])
		df.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))		
		df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])
		df.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ wgts
		df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)    

		# Equal-weighted portfolio
		model = 'est_none'
		cov[0, 0] = cov[1, 1] = cov[2, 2] = (sds.mean())**2
		wgts = (1/n)*np.ones(n)
		# df.loc[i,'portret_'+model]= df.loc[i,['r0','r1','r2']].values @ wgts_est_none
		df.loc[i,'portret_'+model] = df.loc[i,['r0','r1','r2']].values @ wgts
		df.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))
		df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])
		df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)   



	portret_list = ['raver_portret_' +  model for model in model_list]
	stats = df[portret_list].describe()

	sr_df = pd.DataFrame(dtype=float, columns = ['sr'], index = model_list)
	for model in model_list:
		sr_df.loc[model,'sr'] = (stats.loc['mean','raver_portret_' +  model] - r)/stats.loc['std','raver_portret_' +  model]
		
	return sr_df

## Run for a systematic list of inputs (varying window length)
# Took 1 hour to run 10 parms @ 250 sims each
# Risk aversion
raver = 2
# Risk-free rate
r = 0.02
# Investment period
T = 50

# Number of simulations
num_sims = 100

# Asset Parameters
mns1 = np.array([0.06, 0.10, 0.14])
mns2 = np.array([0.08, 0.10, 0.12])

sds1 = np.array([0.16, 0.20, 0.24])

c1 = 0.75

w1 = 10
w2 = 20
w3 = 30
w4 = 40
w5 = 50

mns_dict = {'mns1':mns1, 'mns2':mns2}
sds_dict = {'sds1':sds1}
corr_dict= {'c1':c1}
window_dict = {'w1': w1, 'w2': w2, 'w3': w3, 'w4': w4, 'w5': w5 }

iterables = [list(mns_dict.keys()),
             list(sds_dict.keys()),
             list(corr_dict.keys()), 
             list(window_dict.keys()),
             np.arange(num_sims)]
idx = pd.MultiIndex.from_product(iterables, names=["means", "sds", "corrs", "window", "sim"])
sim_results = pd.DataFrame(dtype='float', columns=['true', 'est_none', 'est_all', 'est_sd_corr', 'est_sd'], index=idx)

for m in list(mns_dict.keys()):
    means = mns_dict[m]
    n = len(means)
    for s in list(sds_dict.keys()):
        sds = sds_dict[s]
        for c in list(corr_dict.keys()):
            corr12 = corr13 = corr23 = corr_dict[c]
            # Covariance matrix
            C  = np.identity(3)
            C[0, 1] = C[1, 0] = corr12
            C[0, 2] = C[2, 0] = corr13
            C[1, 2] = C[2, 1] = corr23
            cov = np.diag(sds) @ C @ np.diag(sds)

            for w in list(window_dict.keys()):

                # print(m + "\t" + s +  "\t" + c + "\t" + w)

                # Run the simulations
                for sim in range(num_sims):
                    # if np.mod(sim,25)==0:
                        # print('Simulation number: ' + str(sim))
                    sim_results.loc[(m,s,c,w,sim)] = simulation(means, cov, short_lb=None, seed=sim, window=window_dict[w]).T.values

stats = sim_results.groupby(['means', 'sds','corrs','window']).mean()
stats = stats[['true','est_all', 'est_sd_corr', 'est_sd','est_none']]

def compare_plot(mns,sds,corr):
    newdf = stats.loc[(mns,sds,corr,slice(None))].stack().reset_index()
    newdf.columns=['window','strategy','sr']
    label_dict = {'true':'True',
                'est_none': 'Est-None',
                'est_all': 'Est-All',
                'est_sd_corr': 'Est-SD-Corr',
                'est_sd': 'Est-SD'}

    newdf['strategy'] = newdf['strategy'].apply(lambda y: label_dict[y])
    newdf['window'] = newdf['window'].apply(lambda y: window_dict[y])
    # newdf
    import plotly.express as px
    fig = go.Figure()
    fig = px.histogram(newdf, x="strategy", y="sr",
                color='window', barmode='group', histfunc='avg',
                height=400)
    fig.layout.yaxis["title"] = "Sharpe ratio"
    fig.layout.xaxis["title"] = "Strategy"             
    fig.show()

compare_plot('mns1','sds1','c1')
```

## Length of Estimation Window (low $E[r]$ dispersion)
```{python}
#| echo: false
compare_plot('mns2','sds1','c1')

```

## Number of Assets

- 3, 5, or 10 assets

- Estimation window of 30 years

- Investment period of 50 years

- Theoretical Sharpe ratio is the same for tangency portfolio

## Number of Assets

```{python}
#| echo: false
import numpy as np
from cvxopt import matrix
from cvxopt.solvers import qp as Solver, options as SolverOptions
from scipy.optimize import minimize_scalar
from scipy.optimize import minimize
import plotly.graph_objects as go
from scipy.stats import multivariate_normal as mvn
import pandas as pd

# Risk aversion
raver = 2

# Risk-free rate
r = 0.02

num_sims = 100
T = 50
win=30


# Simulation function
def simulation(means, cov, short_lb, seed, window):
	rets = mvn.rvs(means, cov, size=window+T, random_state = seed)
	n = len(means)
	return_list = ['r' + str(i) for i in range(n)]
	mean_list  = ['mn' + str(i) for i in range(n)]
	sd_list  = ['sd' + str(i) for i in range(n)]
	corr_list = ['c' + str(i) + str(j) for i in np.arange(n) for j in np.arange(i+1,n)]
	wgt_list = ['wgt' + str(i) for i in range(n)]
	df = pd.DataFrame(data=rets, columns=return_list)

	# Estimate rolling window historical inputs
	for i in np.arange(n):
		df[mean_list[i]] = df[return_list[i]].rolling(window).mean()
	for i in np.arange(n):
		df[sd_list[i]] = df[return_list[i]].rolling(window).std()
	corrs = df[return_list].rolling(window, min_periods=window).corr()
	for i in np.arange(n):
		for j in np.arange(i+1,n):
			df['c'+str(i)+str(j)]=corrs.loc[(slice(None),'r'+str(i)),'r'+str(j)].values
    
	wgts_true = tangency(means,cov,r,short_lb)
	wgt_cal_true = (wgts_true @ means - r) / (raver * (wgts_true @ cov @ wgts_true))


	model_list = ['true', 'est_none', 'est_all', 'est_sd_corr', 'est_sd']
	for model in model_list:
		df['portret_'+model] = np.nan  # portret is the realized portfolio return of the 100% risky asset portfolio
		if model not in ['true','est_none']:
			for wgt in wgt_list:
				df[wgt+'_' + model] = np.nan
		df['wgt_cal_'+model] =np.nan
		df['raver_portret_'+model] =np.nan #raver_portret_ is the realized return of the CAL choice of the raver investor

	for i in np.arange(window,window+T):
		# Full estimation inputs at each point in time
		means = df[mean_list].iloc[i-1].values
		sds   = df[sd_list].iloc[i-1].values
		C = np.identity(n)
		for i2 in np.arange(n):
			for j in np.arange(i2+1,n):
				C[i2,j] = C[j,i2] = df.loc[i-1,'c'+str(i2)+str(j)]
		cov = np.diag(sds) @ C @ np.diag(sds)

		##### Note: all portfolio weights considered to be beginning of period weights
		##### (so multiply by contemporaneous realized returns)
		# Theoretical optimal weights
		model = 'true'
		df.loc[i,'portret_'+model]= df.loc[i,return_list].values @ wgts_true
		df.loc[i,'raver_portret_'+model] = r + wgt_cal_true * (df.loc[i,'portret_'+model] -r)

		# Full estimation tangency portfolio
		model = 'est_all'
		wgts = tangency(means,cov,r,short_lb)
		for a, wgt in enumerate(wgt_list):
			df.loc[i,wgt+'_'+model] = wgts[a]
		df.loc[i,'portret_'+model] = df.loc[i,return_list].values @ wgts
		df.loc[i,'wgt_cal_'+model] = (wgts @ means - r) / (raver * (wgts @ cov @ wgts))
		df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])
		df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)

		# Estimate only covariance matrix
		model = 'est_sd_corr'
		wgts = gmv(cov,short_lb)
		for a, wgt in enumerate(wgt_list):
			df.loc[i,wgt+'_'+model] = wgts[a]
		df.loc[i,'portret_'+model] = df.loc[i,return_list].values @ wgts
		df.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))
		df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])
		df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)


		# Estimate only standard deviations in covariance matrix
		model = 'est_sd'
		for i2 in np.arange(n):
			for j in np.arange(i2+1,n):
				cov[i2,j] = cov[j,i2] =0.0		
		wgts = gmv(cov,short_lb)
		for a, wgt in enumerate(wgt_list):
			df.loc[i,wgt+'_'+model] = wgts[a]
		df.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))		
		df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])
		df.loc[i,'portret_'+model] = df.loc[i,return_list].values @ wgts
		df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)    

		# Equal-weighted portfolio
		model = 'est_none'
		for i2 in np.arange(n):
			cov[i2,i2] = (sds.mean())**2
		wgts = (1/n)*np.ones(n)
		df.loc[i,'portret_'+model] = df.loc[i,return_list].values @ wgts
		df.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))
		df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])
		df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)   



	portret_list = ['raver_portret_' +  model for model in model_list]
	stats = df[portret_list].describe()

	sr_df = pd.DataFrame(dtype=float, columns = ['sr'], index = model_list)
	for model in model_list:
		sr_df.loc[model,'sr'] = (stats.loc['mean','raver_portret_' +  model] - r)/stats.loc['std','raver_portret_' +  model]
		
	return sr_df

# Make theoretical Sharpe ratio constant across cases
mns3 = np.array([0.06, 0.10, 0.14])
mns5 = np.array([0.06, 0.10, 0.14, 0.18, 0.22])
mns10= np.array([0.06, 0.10, 0.14, 0.18, 0.22, 0.06, 0.10, 0.14, 0.18, 0.22])

sds3 = np.array([0.16, 0.20, 0.24])
sds5 = np.array([0.16, 0.20, 0.24, 0.28, 0.32])
sds10= np.array([0.16, 0.20, 0.24, 0.28, 0.32, 0.16, 0.20, 0.24, 0.28, 0.32])

corr = 0.5

mns_dict = {'3':mns3, '5':mns5, '10':mns10}
sds_dict = {'3':sds3, '5':sds5, '10':sds10}

# Adjust covariance matrix so theoretical sharpe ratio is same
sharpes = np.zeros(len(mns_dict.keys()))
for k,key in enumerate(mns_dict.keys()):
    means = mns_dict[key]
    sds   = sds_dict[key]
    n = len(means)
    C = np.identity(n)
    for i in np.arange(0,n):
        for j in np.arange(i+1,n):
            C[i,j] = C[j,i] = corr
    cov = np.diag(sds) @ C @ np.diag(sds)
    wgts_true = tangency(means,cov,r,short_lb=None)
    sr_true = (wgts_true @ means - r) / (np.sqrt(wgts_true @ cov @ wgts_true))
    sharpes[k] = sr_true


iterables = [list(mns_dict.keys()),
             np.arange(num_sims)]
idx = pd.MultiIndex.from_product(iterables, names=["n_assets", "sim"])
sim_results = pd.DataFrame(dtype='float', columns=['true', 'est_none', 'est_all', 'est_sd_corr', 'est_sd'], index=idx)


for k,key in enumerate(mns_dict.keys()):
    # print(key)
    means = mns_dict[key]
    sds   = sds_dict[key]
    n = len(means)
    C = np.identity(n)
    for i in np.arange(0,n):
        for j in np.arange(i+1,n):
            C[i,j] = C[j,i] = corr
    cov = np.diag(sds) @ C @ np.diag(sds)
    cov = cov * (sharpes[k]/sharpes[0])**2

    # Run the simulations
    for sim in range(num_sims):
        # if np.mod(sim,25)==0:
            # print('Simulation number: ' + str(sim))
        sim_results.loc[(key,sim)] = simulation(means, cov, short_lb=None, seed=sim, window=win).T.values

stats = sim_results.groupby(['n_assets']).mean()
stats = stats.reset_index()
stats['num'] = stats['n_assets'].apply(lambda x: int(x))
stats = stats.sort_values('num')
stats = stats[['num','true','est_all', 'est_sd_corr', 'est_sd','est_none']]


# Plot results
import plotly.express as px
newdf = stats.set_index('num').stack().reset_index()
newdf.columns=['# of assets','strategy','sr']
label_dict = {'true':'True',
            'est_none': 'Est-None',
            'est_all': 'Est-All',
            'est_sd_corr': 'Est-SD-Corr',
            'est_sd': 'Est-SD'}

newdf['strategy'] = newdf['strategy'].apply(lambda y: label_dict[y])
fig = go.Figure()
fig = px.histogram(newdf, x="strategy", y="sr",
            color='# of assets', barmode='group', histfunc='avg',
            height=400)
fig.layout.yaxis["title"] = "Sharpe ratio"
fig.layout.xaxis["title"] = "Strategy"             
fig.show()


```

## Empirical Performance: Stocks, Bonds, Gold
```{python}
#| echo: false
import quandl
quandl.ApiConfig.api_key = "f-5zoU2G4zzHaUtkJ7BY"
```

```{python}
#| echo: false

import numpy as np
from cvxopt import matrix
from cvxopt.solvers import qp as Solver, options as SolverOptions
from scipy.optimize import minimize_scalar
from scipy.optimize import minimize
import plotly.graph_objects as go
from scipy.stats import multivariate_normal as mvn
import pandas as pd

# Pull the data (from sbb.py and gold.py from website codebase)
# Stocks, bonds, bills
nominal = pd.read_csv('https://www.dropbox.com/s/hgwte6swx57jqcv/nominal_sbb.csv?dl=1', index_col=['Year'])

# Gold
d = quandl.get("LBMA/GOLD")['USD (AM)']
gold = d.resample('Y').last().iloc[:-1]
gold.index = [x.year for x in gold.index]
gold.loc[1967] = d.iloc[0]
gold = gold.sort_index().pct_change().dropna()
gold.name = 'Gold'
 

df = pd.concat((nominal, gold), axis=1).dropna()
assets = ['TBills','S&P 500', 'Gold', 'Corporates', 'Treasuries']
df = df[assets]

##### Inputs
# Window length (and initial period)
window = 20
n = len(assets)-1
raver = 5
short_lb = None
T = len(df)-window



# Rolling input estimation
risky = assets[1:]
df.columns = ['rf']+['r'+str(i) for i in range(n)]
asset_list = [str(i) for i in range(n)]
for asset in asset_list:
    df['mn' + asset]=df['r'+asset].rolling(window).mean()
    df['sd' + asset]=df['r'+asset].rolling(window).std()

ret_list = ['r' + asset for asset in asset_list]
corrs = df[ret_list].rolling(window, min_periods=window).corr()

corr_list = []
for j, asset in enumerate(asset_list):
    for k in range(j+1,n):
        df['c'+asset+str(k)]=corrs.loc[(slice(None),'r'+asset),'r'+str(k)].values
df['year'] = df.index
df = df.reset_index()


# Prepare columns for the rolling optimization output
model_list = ['ew', 'est_all', 'est_cov', 'est_sds']
for model in model_list:
    df['portret_'+model] = np.nan      #portret is the realized portfolio return of the 100% risky asset portfolio
    if model not in ['ew']:
        for asset in asset_list:
            df['wgt' + asset + '_' +model] = np.nan
    df['wgt_cal_'+model] =np.nan
    df['raver_portret_'+model] =np.nan #raver_portret_ is the realized return of the CAL choice of the raver investor

mn_list = ['mn'+asset for asset in asset_list]
sd_list = ['sd'+asset for asset in asset_list] 

# Choose optimal portfolios each time period
for i in np.arange(window,window+T):
    # Full estimation inputs at each point in time
    means = df[mn_list].iloc[i-1].values
    sds   = df[sd_list].iloc[i-1].values
    C  = np.identity(n)
    for j, asset in enumerate(asset_list):
        for k in range(j+1,n):
            C[j, k] = C[k, j] =    df.loc[i-1,'c'+asset+str(k)]  
    cov = np.diag(sds) @ C @ np.diag(sds)

    r = df.loc[i,'rf']
    ##### Note: all portfolio weights considered to be beginning of period weights
    ##### (so multiply by contemporaneous realized returns)
    # Full estimation tangency portfolio
    model = 'est_all'
    wgts = tangency(means,cov,r,short_lb)
    for j, asset in enumerate(asset_list):
        df.loc[i,'wgt'+asset+'_' + model] = wgts[j]
    df.loc[i,'portret_'+model] = df.loc[i,ret_list].values @ wgts
    df.loc[i,'wgt_cal_'+model] = (wgts @ means - r) / (raver * (wgts @ cov @ wgts))
    df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])
    df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)

    # Estimate only covariance matrix
    model = 'est_cov'
    wgts = gmv(cov,short_lb)
    for j, asset in enumerate(asset_list):
        df.loc[i,'wgt'+asset+'_' + model] = wgts[j]
    df.loc[i,'portret_'+model] = df.loc[i,ret_list].values @ wgts
    df.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))
    df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])
    df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)


    # Estimate only standard deviations in covariance matrix
    model = 'est_sds'
    for j, asset in enumerate(asset_list):
        for k in range(j+1,n):
            cov[j, k] = cov[k, j] = 0.0
    wgts = gmv(cov,short_lb)
    for j, asset in enumerate(asset_list):
        df.loc[i,'wgt'+asset+'_' + model] = wgts[j]
    df.loc[i,'portret_'+model] = df.loc[i,ret_list].values @ wgts
    df.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))		
    df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])
    df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)    

    # Equal-weighted portfolio
    model = 'ew'
    for j, asset in enumerate(asset_list):
        cov[j,j] = (sds.mean())**2
    wgts = (1/n)*np.ones(n)
    df.loc[i,'portret_'+model] = df.loc[i,ret_list].values @ wgts
    df.loc[i,'wgt_cal_'+model] = (means.mean() - r) / (raver * (wgts @ cov @ wgts))
    df.loc[i,'wgt_cal_'+model] = max(0,df.loc[i,'wgt_cal_'+model])
    df.loc[i,'raver_portret_'+model] = r + df.loc[i,'wgt_cal_'+model]  * (df.loc[i,'portret_'+model] -r)   

# Summarize sharpe ratio, avg ret, sd(ret) for each model
portret_list = ['raver_portret_' +  model for model in ['est_all', 'est_cov', 'est_sds','ew']]
stats = df[portret_list].describe()
sr_df = pd.DataFrame(dtype=float, columns = ['sr','avg_ret','sd_ret'], index = ['est_all', 'est_cov', 'est_sds','ew'])
r = df[np.isnan(df['raver_portret_ew'])==False].rf.mean()
for model in ['est_all', 'est_cov', 'est_sds','ew']:
    sr_df.loc[model,'sr'] = (stats.loc['mean','raver_portret_' +  model] - r)/stats.loc['std','raver_portret_' +  model]
    sr_df.loc[model,'avg_ret'] = stats.loc['mean','raver_portret_' +  model]
    sr_df.loc[model,'sd_ret'] = stats.loc['std','raver_portret_' +  model]

label_dict = {'true': 'theoretical optimal weights', 
            'ew': 'equal weights',
            'est_all': 'estimate all inputs',
            'est_cov': 'estimate covariance matrix only',
            'est_sds': 'estimate standard deviations only'}

xaxis_label_dict = {'true': 'theoretical optimal weights', 
            'ew': 'Est-None',
            'est_all': 'Est-All',
            'est_cov': 'Est-SD-Corr',
            'est_sds': 'Est-SD'}
sr_df = sr_df.reset_index()
sr_df['label'] = sr_df['index'].apply(lambda x: label_dict[x])
sr_df['xaxis_label'] = sr_df['index'].apply(lambda x: xaxis_label_dict[x])


# Plot sharpe ratios
string ="Strategy: %{customdata[0]} <br>"
string += "Sharpe ratio: %{y:0.3f}<br>"
string += "Average return: %{customdata[1]:0.1%}<br>"
string += "SD(return): %{customdata[2]:0.1%}<br>"
string += "<extra></extra>"

fig = go.Figure()
fig.add_trace(go.Bar(x=sr_df['xaxis_label'], y=sr_df['sr'], customdata=sr_df[['label','avg_ret','sd_ret']], hovertemplate=string))
fig.layout.yaxis["title"] = "Sharpe ratio"
fig.layout.xaxis["title"] = "Strategy"
fig.show()

```

## Empirical Performance: Stocks, Bonds, Gold
```{python}
#| echo: false
# Plot the time-series of returns and portfolio weights.
for asset in asset_list:
    df['wgt'+asset + '_ew'] = 1/n
       
fig = go.Figure()
for model in ['est_all', 'est_cov', 'est_sds', 'ew']:
    string =  "Strategy: " + label_dict[model] +" <br>"
    string += "Year: %{x:4.0f}<br>"
    string += "Return: %{y:0.1%}<br>"
    string += "Weight in Risky Portfolio: %{customdata[0]: 0.1%} <br>"
    string += "Risky Portfolio Weights:<br>"
    string += "  "+ risky[0] +": %{customdata[1]: 0.1%} <br>"
    string += "  "+ risky[1] +": %{customdata[2]: 0.1%} <br>"
    string += "  "+ risky[2] +": %{customdata[3]: 0.1%} <br>"
    string += "  "+ risky[3] +": %{customdata[4]: 0.1%} <br>"
    string += "<extra></extra>"

    wgt_list = ['wgt_cal_'+ model] + ['wgt'+asset + "_" + model for asset in asset_list]
    trace=go.Scatter(x=df['year'], y=df['raver_portret_'+model], customdata=df[wgt_list], hovertemplate=string, name = xaxis_label_dict[model])
    fig.add_trace(trace)
fig.layout.yaxis["title"] = "Return"
fig.layout.xaxis["title"] = "Year"
fig.update_yaxes(tickformat=".0%")
fig.update_layout(legend=dict(yanchor="top", y =0.99, xanchor="left", x=0.01))
fig.update_xaxes(range=[df['year'].iloc[window], df.year.max()])
fig.show()

```


## Another Solution: Sort on Predictive Signals

We will see this later in the course


## For next time: CAPM

<br><br><br><br>

![](RiceLogo.png){fig-align="center"}